from __future__ import annotations

import json
import os
import re
import shutil
import subprocess
from pathlib import Path
from typing import Any, Literal, cast

import typer
import yaml
from common.run_command import run_command

LOCK_FILE = Path(".env.lock")
LOCAL_LOCK_FILE = Path(".env.local.lock")
COMPOSE_FILE = Path("compose.yml")
BAKE_FILE = Path("compose.bake.yml")
METADATA_PATH = Path("metadata.json")

Mode = Literal["local", "ci"]
Gpu = Literal["auto", "cpu", "cuda", "rocm"]

app = typer.Typer(add_completion=False)


def _load_lock_file(path: Path) -> dict[str, str]:
    return {
        (p := line.split("=", 1))[0].strip(): p[1].strip()
        for line in path.read_text(encoding="utf-8").splitlines()
        if "=" in line and not line.startswith("#")
    }


def _write_lock_file(path: Path, variables: dict[str, str]) -> None:
    path.write_text(
        "# Generated by lock.py\n" + "\n".join(f"{k}={v}" for k, v in sorted(variables.items())) + "\n",
        encoding="utf-8",
    )


def _inspect_image(image_ref: str, log_error: bool = False) -> str | None:
    try:
        output = run_command(
            f"docker buildx imagetools inspect {image_ref}", stream_log=False, log=False, verbose_errors=False
        )
        match = re.search(r"^Digest:\s+(sha256:[a-f0-9]+)", output, re.MULTILINE)
        if match:
            return match.group(1)
    except subprocess.CalledProcessError as e:
        if log_error:
            print(f"    [ERROR] Failed to resolve {image_ref}. Check network/permissions.")
            if e.stderr:
                print(f"    Details: {e.stderr.strip()}")
    return None


def _get_remote_digest(image_ref: str) -> str:
    if "$" in image_ref:
        return ""

    print(f"    Querying registry for {image_ref}...")
    digest = _inspect_image(image_ref, log_error=True)
    if not digest:
        raise RuntimeError(f"Could not resolve digest for required image: {image_ref}")
    return digest


def _check_gc_limits(min_gb: int = 60) -> None:
    candidates = [Path("/etc/docker/daemon.json"), Path(os.path.expanduser("~/.docker/daemon.json"))]
    if "WSL_DISTRO_NAME" in os.environ:
        try:
            win_home = run_command('wslpath $(cmd.exe /c "echo %UserProfile%" 2>/dev/null)', log=False).strip()
            candidates.append(Path(win_home) / ".docker" / "daemon.json")
        except Exception:
            pass

    config = next((p for p in candidates if p.exists() and os.access(p, os.R_OK)), None)
    if not config:
        print(f"    [WARN] No readable daemon.json found. Ensure defaultKeepStorage > {min_gb}GB.")
        return

    data = json.loads(config.read_text())
    raw = data.get("builder", {}).get("gc", {}).get("defaultKeepStorage")
    if not raw:
        raise RuntimeError(f"Missing 'builder.gc.defaultKeepStorage' in {config}. Docker defaults are too low.")

    m = re.match(r"^(\d+(?:\.\d+)?)\s*([TGMK]i?B)?$", str(raw), re.IGNORECASE)
    if not m:
        raise RuntimeError(f"Could not parse 'defaultKeepStorage' value: {raw}")

    mult = {"T": 1024, "G": 1, "M": 1 / 1024, "K": 1 / 1024**2, "B": 1 / 1024**3}
    unit = (m.group(2) or "B")[0].upper()
    val = float(m.group(1)) * mult.get(unit, 1 / 1024**3)

    if val < min_gb:
        raise RuntimeError(
            f"UNSAFE GC LIMIT: {val:.1f}GB in {config} (Required: {min_gb}GB). RESTART DOCKER AFTER FIXING."
        )

    print(f"    [OK] Docker GC Limit verified: {val:.1f}GB")


def _detect_gpu() -> Gpu:
    if shutil.which("nvidia-smi"):
        try:
            run_command("nvidia-smi", stream_log=False, log=False, verbose_errors=False)
            return "cuda"
        except subprocess.CalledProcessError:
            pass
    if shutil.which("rocminfo"):
        try:
            run_command("rocminfo", stream_log=False, log=False, verbose_errors=False)
            return "rocm"
        except subprocess.CalledProcessError:
            pass
    raise RuntimeError("Could not detect GPU type.")


def _bake_targets(
    target_images: dict[str, Any], bake_data: dict[str, Any], no_cache: bool, mode: Mode, gpu: Gpu
) -> None:
    METADATA_PATH.unlink(missing_ok=True)
    bake_targets = [n for n in target_images if n in bake_data.get("services", {})]
    if not bake_targets:
        return

    cmd = [
        "docker buildx bake",
        f"-f {BAKE_FILE}",
        f"--metadata-file {METADATA_PATH}",
        "--progress auto",
        "--provenance=false",
        "--sbom=false",
    ]

    if no_cache:
        cmd.append("--no-cache")

    registry_cache_ref = (
        bake_data.get(f"x-cache-{gpu}", {}).get("x-registry-cache") if gpu in ["cuda", "rocm"] else None
    )

    if mode == "ci":
        # Only push in CI mode
        cmd.append("--push")

        # Alway trying pulling from the GitHub Actions cache for CI builds
        cmd.append(f"--set *.cache-from+=type=gha,scope={gpu}")

        if registry_cache_ref:
            # If this is a GPU build, push only to the registry cache (the GHA cache is too small for GPU builds, and
            # that size cannot be configured, so pushing to it will always cause cache evictions)
            cmd.append(
                f"--set *.cache-to+=type=registry,ref={registry_cache_ref},mode=max,image-manifest=true,oci-mediatypes=true"
            )
        else:
            # Otherwise, push only to the GHA cache
            cmd.append(f"--set *.cache-to+=type=gha,mode=max,scope={gpu}")

    else:
        # Ensure we actually have a local cache directory
        Path(".buildkit-cache").mkdir(exist_ok=True)

        # Pull from and push to the local cache for all local builds
        cmd.append("--load")
        cmd.append("--set *.cache-from+=type=local,src=.buildkit-cache")
        cmd.append("--set *.cache-to+=type=local,dest=.buildkit-cache,mode=max")

    # For both CI and local builds, if this is a GPU build, also pull from the registry cache
    if registry_cache_ref:
        cmd.append(f"--set *.cache-from+=type=registry,ref={registry_cache_ref}")

    # Add targets
    cmd.extend(bake_targets)

    # Bake
    run_command(" ".join(cmd), stream_log=True)


@app.command()
def lock(
    upgrade: bool = typer.Option(False, "--upgrade", "-u", help="Re-resolve and rewrite base digests."),
    mode: Mode = typer.Option("local", "--mode", help="local: updates .env.lock.local; ci: updates .env.lock."),
    gpu: Gpu = typer.Option("auto", "--gpu", help="auto|cpu|cuda|rocm"),
    no_cache: bool = typer.Option(False, "--no-cache", help="Force rebuild by disabling cache usage."),
    targets: list[str] = typer.Option(None, "--target", help="Bake target(s) to build. Overrides --group."),
) -> None:
    if mode not in ["local", "ci"]:
        raise typer.BadParameter("Mode must be 'local' or 'ci'.")
    if mode == "ci" and gpu == "auto":
        raise typer.BadParameter("In CI mode, --gpu cannot be 'auto'; specify 'cpu', 'cuda', or 'rocm'.")

    if not COMPOSE_FILE.exists():
        raise RuntimeError(f"Compose file not found: {COMPOSE_FILE}")
    if not BAKE_FILE.exists():
        raise RuntimeError(f"Build definition file not found at: {BAKE_FILE}")

    # For local builds, ensure Docker GC limits are high enough that GPU builds don't cause cache evictions
    if mode == "local":
        _check_gc_limits(min_gb=60)

    # Resolve gpu
    if gpu == "auto":
        gpu = _detect_gpu()

    # Read bake, compose, and lock files
    bake_data: dict[str, Any] = yaml.safe_load(BAKE_FILE.read_text(encoding="utf-8"))
    compose_data: dict[str, Any] = yaml.safe_load(COMPOSE_FILE.read_text(encoding="utf-8"))
    lock_data = _load_lock_file(LOCK_FILE) if LOCK_FILE.exists() else {}

    # Determine target images to bake
    first_party_images: dict[str, Any] = bake_data["services"]
    if targets:
        invalid = [t for t in targets if t not in first_party_images]
        if invalid:
            raise typer.BadParameter(f"Targets not found or not first-party: {', '.join(invalid)}")
        target_images = {name: first_party_images[name] for name in targets}
    else:
        target_images = {
            name: config
            for name, config in first_party_images.items()
            if not config.get("profiles") or (gpu in config.get("profiles", []))
        }

    # Resolve base image external dependencies
    base_images: dict[str, str] = bake_data["x-base-images"]
    for image, ref in {image: ref for image, ref in base_images.items() if upgrade or image not in lock_data}.items():
        lock_data[image] = f"@{_get_remote_digest(ref)}"

    # Resolve third-party image external dependencies
    third_party_images: dict[str, str] = {
        name.upper().replace("-", "_") + "_IMAGE": config["x-image-ref"]
        for name, config in cast(dict[str, Any], compose_data["services"]).items()
        if name not in bake_data["services"]
    }
    for image, ref in {
        image: ref for image, ref in third_party_images.items() if upgrade or image not in lock_data
    }.items():
        lock_data[image] = f"{ref}@{_get_remote_digest(ref)}"

    # Update main lock file
    _write_lock_file(LOCK_FILE, lock_data)

    # Load local lock file and merge in updated base/third-party image digests from main lock file
    local_lock_data = _load_lock_file(LOCAL_LOCK_FILE) if (mode == "local" and LOCAL_LOCK_FILE.exists()) else {}
    for image, ref in lock_data.items():
        if image not in local_lock_data or image in base_images or image in third_party_images:
            local_lock_data[image] = ref

    # Update environment with updated external dependency image digests
    os.environ.update(local_lock_data)

    # Bake target images
    _bake_targets(target_images, bake_data, no_cache, mode, gpu)

    # Sanity check
    baked_images: dict[str, Any] = json.loads(METADATA_PATH.read_text()) if METADATA_PATH.exists() else {}
    if baked_images.keys() != target_images.keys():
        raise RuntimeError("Baked images do not match target images; something went wrong during the bake.")

    # Lock digests for baked images
    for name in baked_images:
        local_lock_data[name.upper().replace("-", "_") + "_IMAGE"] = (
            f"{baked_images[name]['image.name']}@{baked_images[name]['containerimage.digest']}"
        )

    # Write lock
    _write_lock_file(LOCK_FILE if mode == "ci" else LOCAL_LOCK_FILE, local_lock_data)


def main() -> None:
    app()


if __name__ == "__main__":
    main()
