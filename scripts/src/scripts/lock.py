from __future__ import annotations

import fnmatch
import http.server
import json
import os
import re
import shutil
import socketserver
import subprocess
import sys
import tarfile
import threading
from pathlib import Path
from typing import Any, Literal, cast

import typer
import yaml
from common.run_command import run_command

LOCK_FILE = Path(".env.lock")
LOCAL_LOCK_FILE = Path(".env.local.lock")
COMPOSE_FILE = Path("compose.yml")
BAKE_FILE = Path("compose.bake.yml")
METADATA_PATH = Path("metadata.json")

Mode = Literal["local", "ci"]
Gpu = Literal["auto", "cuda", "rocm"]

app = typer.Typer(add_completion=False, pretty_exceptions_show_locals=False)


class SilentHandler(http.server.SimpleHTTPRequestHandler):
    def log_message(self, format: str, *args: Any) -> None:
        pass  # Keep console clean


def _load_lock_file(path: Path):
    return {
        (p := line.split("=", 1))[0].strip(): p[1].strip()
        for line in path.read_text(encoding="utf-8").splitlines()
        if "=" in line and not line.startswith("#")
    }


def _write_lock_file(path: Path, variables: dict[str, str]):
    path.write_text(
        "# Generated by lock.py\n" + "\n".join(f"{k}={v}" for k, v in sorted(variables.items())) + "\n",
        encoding="utf-8",
    )


def _inspect_image(image_ref: str, log_error: bool = False):
    try:
        output = run_command(
            f"docker buildx imagetools inspect {image_ref}", stream_log=False, log=False, verbose_errors=False
        )
        match = re.search(r"^Digest:\s+(sha256:[a-f0-9]+)", output, re.MULTILINE)
        if match:
            return match.group(1)
    except subprocess.CalledProcessError as e:
        if log_error:
            print(f"    [ERROR] Failed to resolve {image_ref}. Check network/permissions.")
            if e.stderr:
                print(f"    Details: {e.stderr.strip()}")
    return None


def _get_remote_digest(image_ref: str):
    if "$" in image_ref:
        return ""

    print(f"    Querying registry for {image_ref}...")
    digest = _inspect_image(image_ref, log_error=True)
    if not digest:
        raise RuntimeError(f"Could not resolve digest for required image: {image_ref}")
    return digest


def _remote_tag_exists(tag: str) -> bool:
    print(f"    [CHECK] Probing cache: {tag}...")
    try:
        # We use subprocess.run directly to suppress output and capture exit code
        result = subprocess.run(
            ["docker", "manifest", "inspect", tag], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False
        )
        return result.returncode == 0
    except Exception:
        return False


def _check_gc_limits(min_gb: int = 60):
    candidates = [Path("/etc/docker/daemon.json"), Path(os.path.expanduser("~/.docker/daemon.json"))]
    if "WSL_DISTRO_NAME" in os.environ:
        try:
            win_home = run_command('wslpath $(cmd.exe /c "echo %UserProfile%" 2>/dev/null)', log=False).strip()
            candidates.append(Path(win_home) / ".docker" / "daemon.json")
        except Exception:
            pass

    config = next((p for p in candidates if p.exists() and os.access(p, os.R_OK)), None)
    if not config:
        print(f"    [WARN] No readable daemon.json found. Ensure defaultKeepStorage > {min_gb}GB.")
        return

    data = json.loads(config.read_text())
    raw = data.get("builder", {}).get("gc", {}).get("defaultKeepStorage")
    if not raw:
        raise RuntimeError(f"Missing 'builder.gc.defaultKeepStorage' in {config}. Docker defaults are too low.")

    m = re.match(r"^(\d+(?:\.\d+)?)\s*([TGMK]i?B)?$", str(raw), re.IGNORECASE)
    if not m:
        raise RuntimeError(f"Could not parse 'defaultKeepStorage' value: {raw}")

    mult = {"T": 1024, "G": 1, "M": 1 / 1024, "K": 1 / 1024**2, "B": 1 / 1024**3}
    unit = (m.group(2) or "B")[0].upper()
    val = float(m.group(1)) * mult.get(unit, 1 / 1024**3)

    if val < min_gb:
        raise RuntimeError(
            f"UNSAFE GC LIMIT: {val:.1f}GB in {config} (Required: {min_gb}GB). RESTART DOCKER AFTER FIXING."
        )

    print(f"    [OK] Docker GC Limit verified: {val:.1f}GB")


def _detect_gpu() -> Gpu:
    if shutil.which("nvidia-smi"):
        try:
            run_command("nvidia-smi", stream_log=False, log=False, verbose_errors=False)
            return "cuda"
        except subprocess.CalledProcessError:
            pass
    if shutil.which("rocminfo"):
        try:
            run_command("rocminfo", stream_log=False, log=False, verbose_errors=False)
            return "rocm"
        except subprocess.CalledProcessError:
            pass
    raise RuntimeError("Could not detect GPU type.")


def _create_deterministic_info(tar: tarfile.TarFile, path: Path, archive_name: str, mode: int) -> tarfile.TarInfo:
    info = tar.gettarinfo(str(path), arcname=archive_name)
    info.mode = mode
    info.mtime = 0
    info.uid = 0
    info.gid = 0
    info.uname = "root"
    info.gname = "root"
    return info


def _create_deterministic_context(root: Path, output_tar: Path):
    MODE_DIR = 0o755  # rwxr-xr-x
    MODE_FILE = 0o644  # rw-r--r--

    ignore_patterns = [".git", output_tar.name] + [
        line.strip()
        for line in (root / ".dockerignore").read_text(encoding="utf-8").splitlines()
        if line.strip() and not line.startswith("#")
    ]

    print(f"    [CONTEXT] Packing deterministic build context to {output_tar}...")

    with tarfile.open(output_tar, "w") as tar:
        for path, directories, files in os.walk(root):
            directories.sort()
            files.sort()

            relative_path = Path(path).relative_to(root)

            def is_ignored(name: str) -> bool:
                path_string = str(relative_path / name).replace("\\", "/")
                return any(
                    fnmatch.fnmatch(path_string, pattern) or fnmatch.fnmatch(path_string + "/", pattern)
                    for pattern in ignore_patterns
                )

            # Prune ignored directories in place
            directories[:] = [d for d in directories if not is_ignored(d)]

            # Add files
            for name in files:
                if is_ignored(name):
                    continue

                file_path = Path(path) / name

                info = _create_deterministic_info(
                    tar, file_path, str(relative_path / name).replace("\\", "/"), MODE_FILE
                )

                with file_path.open("rb") as f:
                    tar.addfile(info, f)

            # Add directory
            if str(relative_path) != ".":
                tar.addfile(
                    _create_deterministic_info(tar, Path(path), str(relative_path).replace("\\", "/"), MODE_DIR)
                )


@app.command()
def lock(
    upgrade: bool = typer.Option(False, "--upgrade", "-u", help="Re-resolve and rewrite base digests."),
    mode: Mode = typer.Option("local", "--mode", help="local: updates .env.lock.local; ci: updates .env.lock."),
    gpu: Gpu = typer.Option("auto", "--gpu", help="auto|cuda|rocm"),
    no_cache: bool = typer.Option(False, "--no-cache", help="Force rebuild by disabling cache usage."),
) -> None:
    # Read bake, compose, and lock files
    bake_data: dict[str, Any] = yaml.safe_load(BAKE_FILE.read_text(encoding="utf-8"))
    compose_data: dict[str, Any] = yaml.safe_load(COMPOSE_FILE.read_text(encoding="utf-8"))
    lock_data = _load_lock_file(LOCK_FILE) if LOCK_FILE.exists() else {}

    # TOOD: Create separate commands for ci and local modes so typer can do this validation instead of us
    if mode == "ci" and gpu == "auto":
        raise typer.BadParameter("In CI mode, --gpu cannot be 'auto'; specify 'cuda' or 'rocm'.")

    # For local builds, ensure Docker GC limits are high enough that GPU builds don't cause cache evictions
    if mode == "local":
        _check_gc_limits(min_gb=60)

    # Resolve gpu
    if gpu == "auto":
        gpu = _detect_gpu()

    # Resolve base image external dependencies
    base_images: dict[str, str] = bake_data["x-base-images"]
    for image, ref in {image: ref for image, ref in base_images.items() if upgrade or image not in lock_data}.items():
        lock_data[image] = f"@{_get_remote_digest(ref)}"

    # Resolve third-party image external dependencies
    third_party_images: dict[str, str] = {
        name.upper().replace("-", "_") + "_IMAGE": config["x-image-ref"]
        for name, config in cast(dict[str, Any], compose_data["services"]).items()
        if name not in bake_data["services"]
    }
    for image, ref in {
        image: ref for image, ref in third_party_images.items() if upgrade or image not in lock_data
    }.items():
        lock_data[image] = f"{ref}@{_get_remote_digest(ref)}"

    # Update main lock file
    _write_lock_file(LOCK_FILE, lock_data)

    # Load local lock file and merge in updated base/third-party image digests from main lock file
    local_lock_data = _load_lock_file(LOCAL_LOCK_FILE) if (mode == "local" and LOCAL_LOCK_FILE.exists()) else {}
    for image, ref in lock_data.items():
        if image not in local_lock_data or image in base_images or image in third_party_images:
            local_lock_data[image] = ref

    # Update environment with updated external dependency image digests
    os.environ.update(local_lock_data)

    # Build command arguments
    command_arguments: list[str] = []

    # Determine bake targets
    targets = [
        service
        for service in bake_data["services"]
        if not compose_data["services"][service].get("profiles")
        or (gpu in compose_data["services"][service]["profiles"])
    ]

    # Configure registry cache for each target
    for target in targets:
        target_cache = f"{bake_data['x-registry-cache']}:{target}"

        # Only push to the registry cache in CI mode
        if mode == "ci":
            command_arguments.append(
                f"--set {target}.cache-to+=type=registry,ref={target_cache},mode=max,image-manifest=true,oci-mediatypes=true"
            )

        # Add the remote cache as a pull source if it exists (it might not, if this is a new target that has never been built by CI before)
        if _remote_tag_exists(target_cache):
            command_arguments.append(f"--set {target}.cache-from+=type=registry,ref={target_cache}")

    # Load or push images based on mode
    command_arguments.append("--load" if mode == "local" else "--push")

    # Handle no-cache option
    if no_cache:
        command_arguments.append("--no-cache")

    # Append targets
    command_arguments.extend(targets)

    # Clean up any existing metadata file
    METADATA_PATH.unlink(missing_ok=True)

    # Bake images
    context_archive = Path("build_context.tar")
    try:
        # Create deterministic build context
        _create_deterministic_context(Path.cwd(), context_archive)

        is_vm = sys.platform in ("win32", "darwin")

        # Serve context via temporary HTTP server (only way to pass custom context with buildx bake)
        with socketserver.TCPServer(("0.0.0.0" if is_vm else "127.0.0.1", 0), SilentHandler) as httpd:
            server_thread = threading.Thread(target=httpd.serve_forever)
            server_thread.daemon = True
            server_thread.start()

            hostname = "host.docker.internal" if is_vm else "127.0.0.1"
            context_url = f"http://{hostname}:{httpd.server_address[1]}/{context_archive.name}"

            # Bake
            command = [
                "docker buildx bake",
                f"-f {BAKE_FILE}",
                f"--metadata-file {METADATA_PATH}",
                f"--set *.context={context_url}",
                "--progress auto",
                "--provenance=false",
                "--sbom=false",
            ] + command_arguments
            run_command(" ".join(command), stream_log=True)
    finally:
        # Ensure we always clean up the context archive
        context_archive.unlink(missing_ok=True)

    # Sanity check
    baked_images: dict[str, Any] = json.loads(METADATA_PATH.read_text()) if METADATA_PATH.exists() else {}
    if baked_images.keys() != set(targets):
        raise RuntimeError("Baked images do not match target images; something went wrong during the bake.")

    # Lock digests for baked images
    for name in baked_images:
        local_lock_data[name.upper().replace("-", "_") + "_IMAGE"] = (
            f"{baked_images[name]['image.name']}@{baked_images[name]['containerimage.digest']}"
        )

    # Write lock
    _write_lock_file(LOCK_FILE if mode == "ci" else LOCAL_LOCK_FILE, local_lock_data)


def main() -> None:
    app()


if __name__ == "__main__":
    main()
