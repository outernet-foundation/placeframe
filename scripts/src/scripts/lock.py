from __future__ import annotations

import json
import os
import re
import shutil
import subprocess
from pathlib import Path
from typing import Any, Literal, cast

import typer
import yaml
from common.run_command import run_command

LOCK_FILE = Path(".env.lock")
LOCAL_LOCK_FILE = Path(".env.local.lock")
COMPOSE_FILE = Path("compose.yml")
BAKE_FILE = Path("compose.bake.yml")
METADATA_PATH = Path("metadata.json")

Mode = Literal["local", "ci"]
Gpu = Literal["auto", "cuda", "rocm"]

app = typer.Typer(add_completion=False, pretty_exceptions_show_locals=False)


def _load_lock_file(path: Path):
    return {
        (p := line.split("=", 1))[0].strip(): p[1].strip()
        for line in path.read_text(encoding="utf-8").splitlines()
        if "=" in line and not line.startswith("#")
    }


def _write_lock_file(path: Path, variables: dict[str, str]):
    path.write_text(
        "# Generated by lock.py\n" + "\n".join(f"{k}={v}" for k, v in sorted(variables.items())) + "\n",
        encoding="utf-8",
    )


def _inspect_image(image_ref: str, log_error: bool = False):
    try:
        output = run_command(
            f"docker buildx imagetools inspect {image_ref}", stream_log=False, log=False, verbose_errors=False
        )
        match = re.search(r"^Digest:\s+(sha256:[a-f0-9]+)", output, re.MULTILINE)
        if match:
            return match.group(1)
    except subprocess.CalledProcessError as e:
        if log_error:
            print(f"    [ERROR] Failed to resolve {image_ref}. Check network/permissions.")
            if e.stderr:
                print(f"    Details: {e.stderr.strip()}")
    return None


def _get_remote_digest(image_ref: str):
    if "$" in image_ref:
        return ""

    print(f"    Querying registry for {image_ref}...")
    digest = _inspect_image(image_ref, log_error=True)
    if not digest:
        raise RuntimeError(f"Could not resolve digest for required image: {image_ref}")
    return digest


def _remote_tag_exists(tag: str) -> bool:
    print(f"    [CHECK] Probing cache: {tag}...")
    try:
        # We use subprocess.run directly to suppress output and capture exit code
        result = subprocess.run(
            ["docker", "manifest", "inspect", tag], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False
        )
        return result.returncode == 0
    except Exception:
        return False


def _check_gc_limits(min_gb: int = 60):
    candidates = [Path("/etc/docker/daemon.json"), Path(os.path.expanduser("~/.docker/daemon.json"))]
    if "WSL_DISTRO_NAME" in os.environ:
        try:
            win_home = run_command('wslpath $(cmd.exe /c "echo %UserProfile%" 2>/dev/null)', log=False).strip()
            candidates.append(Path(win_home) / ".docker" / "daemon.json")
        except Exception:
            pass

    config = next((p for p in candidates if p.exists() and os.access(p, os.R_OK)), None)
    if not config:
        print(f"    [WARN] No readable daemon.json found. Ensure defaultKeepStorage > {min_gb}GB.")
        return

    data = json.loads(config.read_text())
    raw = data.get("builder", {}).get("gc", {}).get("defaultKeepStorage")
    if not raw:
        raise RuntimeError(f"Missing 'builder.gc.defaultKeepStorage' in {config}. Docker defaults are too low.")

    m = re.match(r"^(\d+(?:\.\d+)?)\s*([TGMK]i?B)?$", str(raw), re.IGNORECASE)
    if not m:
        raise RuntimeError(f"Could not parse 'defaultKeepStorage' value: {raw}")

    mult = {"T": 1024, "G": 1, "M": 1 / 1024, "K": 1 / 1024**2, "B": 1 / 1024**3}
    unit = (m.group(2) or "B")[0].upper()
    val = float(m.group(1)) * mult.get(unit, 1 / 1024**3)

    if val < min_gb:
        raise RuntimeError(
            f"UNSAFE GC LIMIT: {val:.1f}GB in {config} (Required: {min_gb}GB). RESTART DOCKER AFTER FIXING."
        )

    print(f"    [OK] Docker GC Limit verified: {val:.1f}GB")


def _detect_gpu() -> Gpu:
    if shutil.which("nvidia-smi"):
        try:
            run_command("nvidia-smi", stream_log=False, log=False, verbose_errors=False)
            return "cuda"
        except subprocess.CalledProcessError:
            pass
    if shutil.which("rocminfo"):
        try:
            run_command("rocminfo", stream_log=False, log=False, verbose_errors=False)
            return "rocm"
        except subprocess.CalledProcessError:
            pass
    raise RuntimeError("Could not detect GPU type.")


@app.command()
def lock(
    upgrade: bool = typer.Option(False, "--upgrade", "-u", help="Re-resolve and rewrite base digests."),
    mode: Mode = typer.Option("local", "--mode", help="local: updates .env.lock.local; ci: updates .env.lock."),
    gpu: Gpu = typer.Option("auto", "--gpu", help="auto|cuda|rocm"),
    no_cache: bool = typer.Option(False, "--no-cache", help="Force rebuild by disabling cache usage."),
) -> None:
    # Read bake, compose, and lock files
    bake_data: dict[str, Any] = yaml.safe_load(BAKE_FILE.read_text(encoding="utf-8"))
    compose_data: dict[str, Any] = yaml.safe_load(COMPOSE_FILE.read_text(encoding="utf-8"))
    lock_data = _load_lock_file(LOCK_FILE) if LOCK_FILE.exists() else {}

    # TOOD: Create separate commands for ci and local modes so typer can do this validation instead of us
    if mode == "ci" and gpu == "auto":
        raise typer.BadParameter("In CI mode, --gpu cannot be 'auto'; specify 'cuda' or 'rocm'.")

    # For local builds, ensure Docker GC limits are high enough that GPU builds don't cause cache evictions
    if mode == "local":
        _check_gc_limits(min_gb=60)

    # Resolve gpu
    if gpu == "auto":
        gpu = _detect_gpu()

    # Resolve base image external dependencies
    base_images: dict[str, str] = bake_data["x-base-images"]
    for image, ref in {image: ref for image, ref in base_images.items() if upgrade or image not in lock_data}.items():
        lock_data[image] = f"@{_get_remote_digest(ref)}"

    # Resolve third-party image external dependencies
    third_party_images: dict[str, str] = {
        name.upper().replace("-", "_") + "_IMAGE": config["x-image-ref"]
        for name, config in cast(dict[str, Any], compose_data["services"]).items()
        if name not in bake_data["services"]
    }
    for image, ref in {
        image: ref for image, ref in third_party_images.items() if upgrade or image not in lock_data
    }.items():
        lock_data[image] = f"{ref}@{_get_remote_digest(ref)}"

    # Update main lock file
    _write_lock_file(LOCK_FILE, lock_data)

    # Load local lock file and merge in updated base/third-party image digests from main lock file
    local_lock_data = _load_lock_file(LOCAL_LOCK_FILE) if (mode == "local" and LOCAL_LOCK_FILE.exists()) else {}
    for image, ref in lock_data.items():
        if image not in local_lock_data or image in base_images or image in third_party_images:
            local_lock_data[image] = ref

    # Update environment with updated external dependency image digests
    os.environ.update(local_lock_data)

    registry_cache = bake_data["x-registry-cache"]

    # Build the bake command
    command = [
        "docker buildx bake",
        f"-f {BAKE_FILE}",
        f"--metadata-file {METADATA_PATH}",
        "--progress auto",
        "--provenance=false",
        "--sbom=false",
    ]

    # Determine bake targets
    targets = [
        service
        for service in bake_data["services"]
        if not compose_data["services"][service].get("profiles")
        or (gpu in compose_data["services"][service]["profiles"])
    ]

    # Configure cache settings per target
    for target in targets:
        # If the remote cache tag doesn't exist yet (i.e. this is a new target that CI has not built yet), skip it
        target_cache = f"{registry_cache}:{target}"
        if not _remote_tag_exists(target_cache):
            continue

        # Always add the registry cache as a pull source
        command.append(f"--set {target}.cache-from+=type=registry,ref={target_cache}")

        # Only push to the registry cache in CI mode
        if mode == "ci":
            command.append(
                f"--set {target}.cache-to+=type=registry,ref={target_cache},mode=max,image-manifest=true,oci-mediatypes=true"
            )

    # Load or push images based on mode
    command.append("--load" if mode == "local" else "--push")

    # Handle no-cache option
    if no_cache:
        command.append("--no-cache")

    # Append targets
    command.extend(targets)

    # Bake images
    METADATA_PATH.unlink(missing_ok=True)
    run_command(" ".join(command), stream_log=True)
    baked_images: dict[str, Any] = json.loads(METADATA_PATH.read_text()) if METADATA_PATH.exists() else {}

    # Sanity check
    if baked_images.keys() != set(targets):
        raise RuntimeError("Baked images do not match target images; something went wrong during the bake.")

    # Lock digests for baked images
    for name in baked_images:
        local_lock_data[name.upper().replace("-", "_") + "_IMAGE"] = (
            f"{baked_images[name]['image.name']}@{baked_images[name]['containerimage.digest']}"
        )

    # Write lock
    _write_lock_file(LOCK_FILE if mode == "ci" else LOCAL_LOCK_FILE, local_lock_data)


def main() -> None:
    app()


if __name__ == "__main__":
    main()
